{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup: Import Required Libraries"
      ],
      "metadata": {
        "id": "fYZhmrdZkIz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from the standard library\n",
        "# Counter is highly optimized for counting hashable objects (O(n) complexity)\n",
        "from collections import Counter\n",
        "import string"
      ],
      "metadata": {
        "id": "YNf2sNmcjmx7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 1: Employee Performance Bonus Eligibility**\n",
        "## **Optimization:** Uses max() to find the highest score in one pass, and a list comprehension for the second pass to filter names. This avoids sorting (which would be $O(n \\log n)$)."
      ],
      "metadata": {
        "id": "59JPUVVvlG_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_performance_bonus(employees):\n",
        "    \"\"\"\n",
        "    Identifies employees with the highest performance score.\n",
        "    Time Complexity: O(n)\n",
        "    Space Complexity: O(k) where k is number of top performers\n",
        "    \"\"\"\n",
        "    if not employees:\n",
        "        print(\"No employee data available.\")\n",
        "        return\n",
        "\n",
        "    # 1. Find the maximum score (O(n))\n",
        "    max_score = max(employees.values())\n",
        "\n",
        "    # 2. Identify all employees with that score (O(n))\n",
        "    top_performers = [name for name, score in employees.items() if score == max_score]\n",
        "\n",
        "    # Output Formatting\n",
        "    print(f\"Top Performers Eligible for Bonus: {', '.join(top_performers)} (Score: {max_score})\")\n",
        "\n",
        "# Input Data\n",
        "employees = {\n",
        "    \"Ravi\": 92,\n",
        "    \"Anita\": 88,\n",
        "    \"Kiran\": 92,\n",
        "    \"Suresh\": 85\n",
        "}\n",
        "\n",
        "# Execute\n",
        "check_performance_bonus(employees)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUc5PaUuk82M",
        "outputId": "c0d1af48-dff3-4000-e05f-d7602dd89a3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Performers Eligible for Bonus: Ravi, Kiran (Score: 92)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 2: Search Query Keyword Analysis**\n",
        "# **Optimization:** uses str.translate for punctuation removal (faster than regex) and Counter for frequency analysis, resulting in linear $O(n)$ complexity rather than nested loops ($O(n^2)$)."
      ],
      "metadata": {
        "id": "UNx8OnrIrEix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_search_query(query):\n",
        "    \"\"\"\n",
        "    Analyzes keyword frequency in a search query.\n",
        "    Time Complexity: O(n) where n is string length\n",
        "    Space Complexity: O(m) where m is unique words\n",
        "    \"\"\"\n",
        "    # 1. Preprocessing: Lowercase and remove punctuation efficiently\n",
        "    # maketrans creates a mapping table to remove punctuation characters\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    clean_query = query.lower().translate(translator)\n",
        "\n",
        "    # 2. Tokenize\n",
        "    words = clean_query.split()\n",
        "\n",
        "    # 3. Count frequencies (O(n))\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # 4. Filter for keywords appearing > 1\n",
        "    result = {word: count for word, count in word_counts.items() if count > 1}\n",
        "\n",
        "    print(result)\n",
        "\n",
        "# Input Data\n",
        "query_input = \"Buy mobile phone buy phone online\"\n",
        "\n",
        "# Execute\n",
        "analyze_search_query(query_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uprQ9IGkrmQh",
        "outputId": "9852f8e2-5242-4ed1-f005-894119b92367"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'buy': 2, 'phone': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 3: Sensor Data Validation**\n",
        "## **Optimization:** Uses enumerate within a list comprehension. This is more memory efficient and faster than creating a separate index list or using a standard for loop with append."
      ],
      "metadata": {
        "id": "8nEGuxBy46FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_sensor_data(readings):\n",
        "    \"\"\"\n",
        "    Filters valid (even) sensor readings with their indices.\n",
        "    Time Complexity: O(n)\n",
        "    Space Complexity: O(k) where k is number of valid readings\n",
        "    \"\"\"\n",
        "    # List comprehension to filter even numbers and capture index\n",
        "    # Format: (hour_index, reading_value)\n",
        "    valid_readings = [(index, value) for index, value in enumerate(readings) if value % 2 == 0]\n",
        "\n",
        "    print(\"Valid Sensor Readings (Hour, Value):\")\n",
        "    print(valid_readings)\n",
        "\n",
        "# Input Data\n",
        "sensor_readings = [3, 4, 7, 8, 10, 12, 5]\n",
        "\n",
        "# Execute\n",
        "validate_sensor_data(sensor_readings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH6MFC7dx16C",
        "outputId": "2c13279c-96a8-4582-a63b-cceb57ba05fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Sensor Readings (Hour, Value):\n",
            "[(1, 4), (3, 8), (4, 10), (5, 12)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 4: Email Domain Usage Analysis**\n",
        "## **Optimization:** Extracts domains in a single pass and uses Counter to aggregate. This avoids multiple passes over the list."
      ],
      "metadata": {
        "id": "rouL5A-d8_S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_email_domains(email_list):\n",
        "    \"\"\"\n",
        "    Calculates percentage usage of email domains.\n",
        "    Time Complexity: O(n)\n",
        "    Space Complexity: O(d) where d is unique domains\n",
        "    \"\"\"\n",
        "    if not email_list:\n",
        "        return\n",
        "\n",
        "    # 1. Extract domains using string splitting\n",
        "    domains = [email.split('@')[1] for email in email_list]\n",
        "\n",
        "    # 2. Count domains\n",
        "    domain_counts = Counter(domains)\n",
        "    total_emails = len(email_list)\n",
        "\n",
        "    # 3. Calculate percentage and display\n",
        "    for domain, count in domain_counts.items():\n",
        "        percentage = (count / total_emails) * 100\n",
        "        print(f\"{domain}: {int(percentage)}%\")\n",
        "\n",
        "# Input Data\n",
        "emails = [\n",
        "    \"ravi@gmail.com\",\n",
        "    \"anita@yahoo.com\",\n",
        "    \"kiran@gmail.com\",\n",
        "    \"suresh@gmail.com\",\n",
        "    \"meena@yahoo.com\"\n",
        "]\n",
        "\n",
        "# Execute\n",
        "analyze_email_domains(emails)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDfJkDtB8jpk",
        "outputId": "c9bb1919-7816-4d5d-e3aa-5def44ea7b8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gmail.com: 60%\n",
            "yahoo.com: 40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 5: Sales Spike Detection**\n",
        "## **Optimization:** Calculates the average first (one pass), then filters for spikes (second pass). This is strictly $O(n)$."
      ],
      "metadata": {
        "id": "dTbC5v4DCB6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_sales_spikes(sales_data):\n",
        "    \"\"\"\n",
        "    Detects sales days that are 30% higher than the average.\n",
        "    Time Complexity: O(n)\n",
        "    \"\"\"\n",
        "    if not sales_data:\n",
        "        return\n",
        "\n",
        "    # 1. Calculate Average\n",
        "    average_sales = sum(sales_data) / len(sales_data)\n",
        "\n",
        "    # 2. Define Threshold (30% above average)\n",
        "    threshold = average_sales * 1.3\n",
        "\n",
        "    # 3. Detect Spikes (using enumerate for Day number)\n",
        "    # Note: Day is index + 1\n",
        "    for day, sale in enumerate(sales_data):\n",
        "        if sale > threshold:\n",
        "            print(f\"Day {day + 1}: {sale}\")\n",
        "\n",
        "# Input Data\n",
        "sales = [1200, 1500, 900, 2200, 1400, 3000]\n",
        "\n",
        "# Execute\n",
        "detect_sales_spikes(sales)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXavJYI7BuCQ",
        "outputId": "76611619-c0f7-4191-f444-c94a10a5d6ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Day 6: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 6: Duplicate User ID Detection**\n",
        "## **Optimization:** Standard nested loops would be $O(n^2)$. Using a Hash Map (via Counter) reduces this to $O(n)$ time complexity, which is crucial for large user databases."
      ],
      "metadata": {
        "id": "cj4PYQphFyOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_duplicate_ids(user_list):\n",
        "    \"\"\"\n",
        "    Identifies duplicate user IDs and their counts.\n",
        "    Time Complexity: O(n)\n",
        "    Space Complexity: O(u) where u is unique users\n",
        "    \"\"\"\n",
        "    # Count occurrences of each ID\n",
        "    id_counts = Counter(user_list)\n",
        "\n",
        "    # Filter and display only duplicates\n",
        "    for user, count in id_counts.items():\n",
        "        if count > 1:\n",
        "            print(f\"{user} -> {count} times\")\n",
        "\n",
        "# Input Data\n",
        "user_ids = [\"user1\", \"user2\", \"user1\", \"user3\", \"user1\", \"user3\"]\n",
        "\n",
        "# Execute\n",
        "find_duplicate_ids(user_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTX2uyPpFvTs",
        "outputId": "6c7d6c28-e4e1-4682-ff52-6ac097cd4d7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user1 -> 3 times\n",
            "user3 -> 2 times\n"
          ]
        }
      ]
    }
  ]
}